# Thalassa - Sakarya University AI Assistant

Thalassa is an AI-powered chatbot designed to assist Sakarya University students by providing clear and concise answers based on university-related information. It utilizes Retrieval-Augmented Generation (RAG) with Ollama (Mistral-7B) and FAISS to deliver contextually relevant responses.

## Overview

Thalassa leverages a local Large Language Model (LLM) accessed via Ollama, combined with a vector database (FAISS) containing indexed information extracted from university documents. When a user asks a question, the system:

1.  Detects the language (Turkish/English).
2.  Translates the query to English if necessary (for optimal embedding model performance).
3.  Searches the FAISS index for relevant text chunks (context).
4.  Constructs a prompt including the retrieved context and the user's query.
5.  Sends the prompt to the Ollama API (Mistral model) to generate an answer.
6.  Translates the answer back to the original language (if necessary).
7.  Returns the final response to the user.

## Features

- **AI-Powered Responses:** Uses Mistral-7B via Ollama for natural language understanding and generation.
- **Retrieval-Augmented Generation (RAG):** Employs FAISS and Sentence Transformers (`all-MiniLM-L6-v2`) to find relevant information from source documents, improving answer accuracy and relevance.
- **Multilingual Support:** Automatically detects user query language (Turkish/English) and provides responses in the same language.
- **Context-Aware:** Uses retrieved text chunks as context for the LLM.
- **User-Friendly Interface:** Simple React-based chat interface for interaction.
- **Asynchronous Processing:** Handles requests efficiently using FastAPI.
- **Input Validation:** Limits message length to prevent excessively long queries.
- **Loading Indicator:** Provides visual feedback while the AI is generating a response.
- **Welcome Modal:** Informs users about potential AI inaccuracies.

## Technology Stack

- **Backend:**
  - Python 3.x
  - FastAPI (Web framework)
  - Ollama (LLM serving)
  - FAISS (Vector similarity search)
  - Sentence Transformers (Text embeddings)
  - Langdetect (Language detection)
  - Requests (HTTP requests)
  - Uvicorn (ASGI server)
  - python-dotenv (Environment variables)
- **Frontend:**
  - React
  - JavaScript
  - Material UI (MUI) (UI components)
  - Axios (HTTP client)
  - React Spinners (Loading indicator)
  - Lodash.debounce (Input debouncing)
- **AI Model:** Mistral-7B (or other Ollama-compatible models)
- **Vector Store:** FAISS

## Project Structure

.
├── app/ # FastAPI backend source code
│ ├── init.py
│ ├── ai_response.py # Handles generation of AI response via Ollama
│ ├── faiss_search.py # Handles FAISS index searching
│ ├── main.py # FastAPI application entry point and API endpoints
│ └── translation.py # Handles language detection and translation
├── data/ # Stores generated data files
│ ├── faiss_index.bin # FAISS index file
│ └── metadata.npy # Metadata corresponding to index entries
├── extracted_texts/ # Folder containing source .txt files
│ ├── doc1.txt
│ ├── doc2.txt
│ └── ...
│ └── hepsi.txt # Merged text file (generated by script)
├── frontend/ # React frontend source code (assuming this structure)
│ ├── public/
│ │ └── thalassa.png
│ │ └── index.html
│ ├── src/
│ │ ├── components/ # React UI components
│ │ ├── hooks/ # Custom React hooks (e.g., useChat)
│ │ ├── styles/ # CSS/styling files
│ │ ├── App.js # Main application component
│ │ └── index.js # React entry point
│ ├── package.json # Frontend dependencies
│ └── ...
├── utils/ # Utility scripts
│ ├── create_faiss_index.py # Script to create the FAISS index
│ └── merge_txt_files.py # Script to merge text files
├── venv/ # Python virtual environment (created by user)
├── .env # Environment variables (OLLAMA_API_URL)
├── requirements.txt # Backend Python dependencies
├── reload.sh # Script to rebuild the FAISS index
└── run.sh # Script to start the backend services

## Setup and Installation

1.  **Clone the Repository:**

    ```bash
    git clone <your-repository-url>
    cd <repository-directory>
    ```

2.  **Install Ollama:**

    - Follow the instructions on the [Ollama website](https://ollama.com/) to install Ollama for your operating system.
    - Pull the Mistral model:
      ```bash
      ollama pull mistral
      ```

3.  **Backend Setup:**

    - Ensure you have Python 3.8+ installed.
    - Create and activate a virtual environment:
      ```bash
      python -m venv venv
      source venv/bin/activate  # On Windows: venv\Scripts\activate
      ```
    - Install Python dependencies:
      ```bash
      pip install -r requirements.txt
      ```
    - Create a `.env` file in the project root and add the Ollama API URL:
      ```env
      OLLAMA_API_URL=http://localhost:11434/api/generate
      ```
    - Place your source `.txt` files (containing the information for the chatbot) into the `extracted_texts/` directory.
    - Prepare the data and build the FAISS index:
      ```bash
      bash reload.sh
      ```
      _(This script deletes old index files, merges text files, and creates a new FAISS index)_

4.  **Frontend Setup:**
    - Ensure you have Node.js and npm (or yarn) installed.
    - Navigate to the frontend directory (e.g., `cd frontend`).
    - Install Node.js dependencies:
      ```bash
      npm install
      # or
      yarn install
      ```

## Usage

1.  **Start the Backend:**

    - Open a terminal in the project root directory.
    - Make sure your virtual environment is activated (`source venv/bin/activate`).
    - Run the `run.sh` script. This will start the Ollama service (if not already running) and the FastAPI server.
      ```bash
      bash run.sh
      ```
      The FastAPI server will usually run on `http://localhost:8000`.

2.  **Start the Frontend:**

    - Open another terminal.
    - Navigate to the frontend directory (e.g., `cd frontend`).
    - Start the React development server:
      ```bash
      npm start
      # or
      yarn start
      ```
    - Open your web browser and go to `http://localhost:3000` (or the port specified by the React development server).

3.  **Interact:**
    - You should see the Thalassa chat interface.
    - Type your questions related to Sakarya University in the input field and press Enter or click "Send".

## Scripts

- **`run.sh`:**
  - Activates the Python virtual environment.
  - Starts the Ollama server in the background (`nohup ollama serve &`).
  - Starts the FastAPI backend server using Uvicorn with auto-reload enabled (`uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload`).
- **`reload.sh`:**
  - Deletes existing FAISS index (`.bin`) and metadata (`.npy`) files from the `data/` folder.
  - Runs `utils/merge_txt_files.py` to combine all `.txt` files in `extracted_texts/` into `hepsi.txt`.
  - Runs `utils/create_faiss_index.py` to generate embeddings and create/save the FAISS index and metadata based on the content in `extracted_texts/`.
  - Run this script whenever you add, remove, or modify the source text files in `extracted_texts/`.

## Configuration

- **`OLLAMA_API_URL`:** Defined in the `.env` file. Specifies the endpoint for the Ollama API generate function. Defaults to `http://localhost:11434/api/generate` if not set.
- **`CHUNK_SIZE`:** Defined in `utils/create_faiss_index.py`. Controls the size (in words) of the text chunks indexed by FAISS.
- **`MODEL_NAME`:** Defined in `utils/create_faiss_index.py`. Specifies the Sentence Transformer model used for embeddings.
- **`INDEX_FILE`, `METADATA_FILE`, `TEXT_FOLDER`:** Path configurations defined in multiple Python files (`app/faiss_search.py`, `utils/create_faiss_index.py`). Ensure consistency if modifying these paths.
